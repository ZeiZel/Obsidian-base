
## О приложении

Приложение для сокращения ссылок:
- app производит сокращение
- api общается с app
- postgresql занимается хранением данных о ссылках 
- `create` / `delete` / `getAll` - создают, удаляют и получают все ссылки
- `/<rest>` - тут уже происходит редирект

![](_png/Pasted%20image%2020250622092936.png)

## Первый POD

 Главное отличие pod от контейнера заключается в том, что это может быть абстракция над несколькими контейнерами. То есть мы можем запустить контейнер с PGSQL и контейнер, который будет его бэкапить.  

![](_png/Pasted%20image%2020250622095427.png)

Далее укажем такую конфигурацию, в которой будет:
- находиться наш собственный image с docker-образом приложения
- в `ports` укажем `containerPort`, который нам нужно прокинуть наружу, чтобы иметь доступ для получения статики сайта от NGINX
- обязательно указываем `resources`, в котором у нас стоят лимиты, при достижении которых, наш сервис будет перезапущен

`pod.yml`
```YAML
---
apiVersion: v1
kind: Pod
metadata:
  name: short-app
  labels:
    components: frontend
spec:
  containers:
    - name: short-app
      image: antonlarichev/short-app
      ports:
        - containerPort: 80 # прокидываем наружу порт контейнера с NGINX
      # обязательно указываем максимальные ресурсы, которые может кушать контейнер
      resources:
        limits:
          memory: "128mi" # mi - mb
          cpu: "500m" # m - miliprocessors - величина относительная к процессору, но указывает количество доступной нагрузки
```

## Сервис

Сервис позволяет держать с контейнером постоянную связь. Без сервиса нам придётся полчать связь каждый раз по IP, который может в любой момент поменяться после редеплоя , если сервис уйдёт в memory leak или упадёт. 

Сервисы бывают 4ёх типов:
- Ingress - позволяет получить доступ к контейнеру извне
- NodePort - позволяет прокинуть порт изнутри контейнера наружу 
- ClusterIP
- LoadBalancer - балансирует нагрузку

Пользователь обращается к IP-адресу кластера, где запрос уходит на прокси, который проксирует все запросы от кластера до определённого сервиса. Потом на сервис, который прокидывает порт из POD в мир. 
 
![](_png/Pasted%20image%2020250622154540.png)

Kube-Proxy требует, чтобы в конфиге было понятно, откуда и куда какой порт проксировать. Поэтому нам нужно будет указать в спеках порты прохода запросов.

- `nodePort` говорит, что при обращении к кластеру по данному порту, мы должны прокинуть запрос в определённый сервис (в котором мы указали этот порт)
- `targetPort` говорит нам на какой порт пода мы должны стучаться, когда уже попали в сервис
- `port` - это внутренний порт для кластера, по которому другие поды смогут достучаться до нашего пода

Для доступа извне нам нужна связка `nodePort` и `targetPort`. В нашем случае, `targetPort` будет равен `containerPort` из конфига пода, так как через таргетпорт нам нужно достучаться до приложения извне кластера 

![](_png/Pasted%20image%2020250622155305.png)

Дальше нам останется только привязать сервис к определённому поду через селектор. В качестве селектора будет выступать заданный ранее лейбл в поде

`node-port.yml`
```YAML
---
apiVersion: v1
kind: Service
metadata:
  name: short-app-port
spec:
  type: NodePort
  # порты для доступа к контейнеру извне кластера
  ports:
   - port: 3000
     targetPort: 80
     nodePort: 31200
  # селектор для связи пода и сервиса
  selectors:
    components: frontend
```

## Подключение к контейнеру

Через утилиту `kubectl` мы можем выполнять все операции по работе с кластером.

### Применение конфига

Через `apply` мы можем применить определённый конфиг к нашему кластеру для выполнения. 

```bash
# запускаем все файлы в папке
kubectl apply -f .

# либо можно запустить отдельный файл
kubectl apply -f pod.yml
```

### Инспект конфига

Через `get` мы можем проинспектировать определённый элемент нашей системы.

`all` позволяет получить все возможные образы, которые были запущены ранее в определённом кластере

```bash
$ kubectl get all

NAME            READY   STATUS         RESTARTS   AGE
pod/short-app   0/1     ErrImagePull   0          13s

NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          5m25s
service/short-app-port   NodePort    10.110.44.54   <none>        3000:31200/TCP   4s
```

Так же ничто нам не мешает вывести только поды

```bash
$ kubectl get pods

NAME        READY   STATUS    RESTARTS   AGE
short-app   1/1     Running   0          3m43s
```

Ну и так же отдельно можно взглянуть на запущенные сервисы

```bash
$ kubectl get services

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          11m
short-app-port   NodePort    10.110.44.54   <none>        3000:31200/TCP   6m8s
```

### Коннект к контейнеру

Чтобы отправить запрос в кластер, нам нужно будет получить ip кластера, который мы создали через `minikube`

```bash
$ minikube ip

192.168.58.2
```

Теперь можно по `http://192.168.58.2:31200` получить доступ к фронту, который был поднят из кубера

![](_png/Pasted%20image%2020250622164853.png)

## Как работает запуск

`kubectl` отправил запрос в Master ноду в API Service и передал в него конфиг

![](_png/Pasted%20image%2020250622164927.png)

Дальше планировщик ищет ноду, где он сможет запустить наш конфиг `short-app`. У нас пока одна нода. 

![](_png/Pasted%20image%2020250622170107.png)

Дальше уже Controller Manager зафиксировал тот факт, что у нас должен быть один инстанс `short-app`. 
Если наше приложение упадёт, то CM попросит запланировать Scheduler снова поднять контейнер в ноде.

На этом же шаге kubelet (администрирующий нашу ноду) стягивает image из dockerhub и поднимает POD

![](_png/Pasted%20image%2020250622170338.png)

Со стороны Pod у нас всего несколько этапов:
- Scheduled - запланирован 
- Pull - стягивает image, если его нет локально, чтобы запустить 
- Create - создаётся Pod
- Start - запускается Pod с контейнером

Чтобы получить полностью всю информацию о запуске пода, мы можем воспользоваться `kubectl describe`, где есть вся информация по подах
 
```bash
$ kubectl describe pods short-app

Name:             short-app
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.58.2
Start Time:       Sun, 22 Jun 2025 16:30:10 +0300
Labels:           components=frontend
Annotations:      <none>
Status:           Running
IP:               10.244.0.3
IPs:
  IP:  10.244.0.3
Containers:
  short-app:
    Container ID:   docker://465ab518889b248c72f6aaad19df0bef319f53a7edc3d9ff4fd105e2e9b1d378
    Image:          antonlarichev/short-app
    Image ID:       docker-pullable://antonlarichev/short-app@sha256:ecf6b7afbfc7b40b27516953c5dffc7325d5fe95ce811f43faa064ed2c86dcd9
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 22 Jun 2025 16:30:39 +0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        500m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4c4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-jr4c4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  40m                default-scheduler  Successfully assigned default/short-app to minikube
  Warning  Failed     40m                kubelet            Failed to pull image "antonlarichev/short-app": Error response from daemon: Get "https://registry-1.docker.io/v2/": tls: received record with version 857 when expecting version 303
  Warning  Failed     40m                kubelet            Error: ErrImagePull
  Normal   BackOff    40m                kubelet            Back-off pulling image "antonlarichev/short-app"
  Warning  Failed     40m                kubelet            Error: ImagePullBackOff
  Normal   Pulling    40m (x2 over 40m)  kubelet            Pulling image "antonlarichev/short-app"
  Normal   Pulled     40m                kubelet            Successfully pulled image "antonlarichev/short-app" in 14.328s (14.328s including waiting). Image size: 109145336 bytes.
  Normal   Created    40m                kubelet            Created container: short-app
  Normal   Started    40m                kubelet            Started container short-app
```

Уже ближе к концу виднеется поле Events, в котором описаны все операции и ошибки, которые могли произойти во время сборки. Тут можно заметить, что первые несколько операций прошли с ошибкой и не получилось сразу стянуть с registry валидные данные. Потом уже только после бэк-оффа произошёл пуллинг.

Так же мы можем проинспектировать и сервис

```bash
$ kubectl describe service short-app-port

Name:                     short-app-port
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 components=frontend
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.110.44.54
IPs:                      10.110.44.54
Port:                     <unset>  3000/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  31200/TCP
Endpoints:                10.244.0.3:80
Session Affinity:         None
External Traffic Policy:  Cluster
Internal Traffic Policy:  Cluster
Events:                   <none>
```



