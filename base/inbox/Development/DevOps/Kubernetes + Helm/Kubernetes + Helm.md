---
tags:
  - kubernetes
  - helm
  - devops
---

---
## Введение

Как выглядит деплой современного приложения:
1. У нас есть готовый продукт, который мы билдим в приложение
2. Далее нам нужно собрать из нашего приложения контейнер
3. Сам контейнер будет попадать в наш registry, где будет сохраняться версия нашей сборки
4. Далее уже большое количестворазных образов оркестрируется с помощью kubernetes, либо с помощью немного устаревшего swarm

![](_png/Pasted%20image%2020240821203021.png)

Swarm - это простое ручное решение для поднятия сразу нескольких контейнеров. Его проще поднять, а так же он является нативным решением. Основным его недостатком являются меньшие возможности относительно кубера и отсутсивие динамического масшатбирования проекта, если у нас будет не хватать мощностей.

Kubernetes - это более сложное решение для оркестрации большого количества контейнеров. У него есть множество готовых решений у различных провайдеров. Он чаще встречается на различных проектах, а так же он достаточно гибок в расширении и обновлении контейнеров. Он может предоставить нам инструментарии бесшовного релиза приложения без его остановки, что не может дать docker без ansible.

![](_png/Pasted%20image%2020240720070535.png)



---
## Настройка окружения


То, что находится на облаках и локально - это разные вещи. Локально находится только малая база. 

- Kubectl - CLI для взаимодействия
- VM driver - виртуальная машина для запуска
- minikube - запускает одну ноду на VM и управляет ей

Minikube имеет огромное количество различных драйверов, через которые можно запустить VM

![](_png/Pasted%20image%2020250323192647.png)

Установить всё дело [можно и вручную по документации](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/), но куда проще будет через homebrew

```bash
brew install kubectl minikube qemu
```

Далее запускаем конфигурацию миникуба

```bash
minikube start --driver qemu
# либо
minikube start --driver docker
```

И получаем список всех досутпных виртуалок

```bash
$ kubectl get all

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   83d
```



---
## Знакомство с kubernetes


### Разные окружения

В рамках локальной разработки мы через kubectl обращаемся к виртуальной машине, которую мы настроили через minikube, и управляем ей. 

В проде же у нас будет уже развёрнутый кластер виртуальных машин в стороннем облаке. Там уже будет находиться manage cluster, с которым мы сможем работать через kubectl. Можно и самому его настраивать, но без острой необходимости для работы большой системы - это малонеобходимо. 

![](_png/Pasted%20image%2020250324182704.png)

### Компоненты

Представим, что у нас появилась достаточно высокая нагрузка на отдельный сервис в рамках отдельной ноды

![](_png/Pasted%20image%2020250324183542.png)

Если мы пойдём по классическому пути кластеризации, то мы можем поднять рядом в отдельной ноде инстанс наших микросервисов. Однако такой подход не будет эффективным, так как мы заняли много ресурсов вне нужного нам сервиса 

![](_png/Pasted%20image%2020250324183553.png)

Через кубер у нас появится возможность поднять целую ноду определённого сервиса и легко отмасштабировать его, если появится большая лишняя нагрузка на сервис

![](_png/Pasted%20image%2020250324183648.png)

Pod - это самая маленькая единица в kubernetes, которая представляет из себя контейнер со своим IP и является абстракцией над определённым контейнером

> Pod может из себя предствлять отдельный контейнер с NGinx, который будет отдавать статику сайта

![](_png/Pasted%20image%2020250324184043.png)

Service - это элемент, который позволяет обеспечить постоянный доступ к котейнеру

> Контейнер имеет memory leak и при достижении порога потребляемой памяти, этот контейнер будет уничтожен

Каждому поду выделяется свой IP в рамках ноды и при пересоздании пода (упал/поднялся), ему переприсваивается другой IP. Отслеживанием обновления адресов и другой информации занимается сервис

![](_png/Pasted%20image%2020250324184417.png)

Всего есть два типа сервисов:
- NodePort - обращается к контейнеру по порту (не самый удобный вариант)
- Ingress - обращается извне к поду по домену

![](_png/Pasted%20image%2020250324184838.png)

### Устройство kubernetes

Node - это виртуальная машина или сервер, которая выполняет одну из двух ролей:

1. Master Node - управляет работой подов
2. Worker Node - хранит поды, которвые непосредственно выполняются

> В рамках minikube одна нода выполняет все функции 

**Kubectl** - это одна из утилит, которая может обращаться к куберу. Когда мы работаем с кластером через интерфейс, мы отправляем запросы в **мастер ноду**, которая управляет остальными подами. Через kubectl мы отправляем запросы на **API Server**, который находится в мастер ноде.

Если мы отправляем запрос на добавление ещё одного пода, наша операция попадает в **Scheduler**, который планирует размещение подов в нодах. **Планировщик** отправляет задание ноде поднять нужный нам под. 

Само задание, которое мы получили из мастер ноды от **планировщика**, выполняет **kubelet**. Это процесс, который непосредственно вызывает поднятие пода с нужным контейнером. 

Сам **планировщик** имеет информацию о нагрузке каждой ноды и отправляет задание в наименее нагруженную. 

Кроме планировщика нам может потребоваться контроллер, который будет управлять жизненным циклом кластера. Если какой-то под упал, то его нужно будет поднять и максимально сохранять заданную конфигурацию кубера, которая была задана человеком. В этом случае нам может понадобиться **Controller Manager**, который контролирует как поды, так и целые ноды, умея перераспределять их, если какие-то из серверов вышли из строя.

Всю информацию каждый элемент не хранит в себе, а берёт и записывает в **Key-Value Storage**, в котором находится информация о нодах, подах и их метаданных.

![](_png/Pasted%20image%2020250324190621.png)

Так же у нас имеются и другие элементы системы, как Volumes, Configs, Secrets и так далее.

Мастер нод так же может быть несколько.

### Разные подходы

При работе с Kubectl у нас есть несколько подходов к работе с ним

#### Императивный подход

Мы явно указываем то, что мы хотим сделать

Тут мы напрямую взаимодействуем с кластером и просим поднять сервер nginx, который никогда не будет перезапускаться

```bash
kubectl run nginx --image=nginx --restart=Never
```

![](_png/Pasted%20image%2020250324191105.png)

#### Декларативный подход

Мы указываем конечный результат, которого хотим добиться. 

В таком случае мы идём от конфига или идеального вида системы, которого хотим добиться

![](_png/Pasted%20image%2020250324191101.png)

#### Когда применять?

Императивный для коротких операций и тестирования, а декларативный нужно применять для поддержки IaC, историчности и для работы с командой разработчиков

![](_png/Pasted%20image%2020250324191301.png)

### Конфигурации

Конфигурация Kubernetes делится на две части: метаданные и спецификация

1. Метаданные

- `apiVersion` - версия используемого API
- `kind` - это тип контейнера, котрый мы описываем (Pod, Service, Node)
- `metadata` - метаданные по описываемому сервису, которые нужны для связи объектов друг с другом

2. Спецификация

Её ключи зависят от типа `kind`, который мы указали, так как для каждого типа контейнера будут свои поля

- `spec` - обозначение начала спецификации
- `containers` - список объектов с описанием контейнеров
- `name` / `image` / `imagePullPolicy` - конфигурации контейнера (имя, изображение, политика пуллинга изображеия)
- `nodeSelector` - 
- `disktype` - тип дисков, на которых должна запуститься нода

![](_png/Pasted%20image%2020250324193256.png)

`apiVersion` принимает два параметра: `v1` и `apps/v1`. Второй нужен для того, чтобы управлять другими объектами.

![](_png/Pasted%20image%2020250324193927.png)

Kubernetes парсит конфиг и все настройки, которые мы описали, складывает в etcd хранилище, которое используется другими компонентами системы для построения всех нод и подов и сведение их с описанной структурой из конфигурации

![](_png/Pasted%20image%2020250324194047.png)



---
## Первый pod

### О приложении

Приложение для сокращения ссылок:
- app производит сокращение
- api общается с app
- postgresql занимается хранением данных о ссылках 
- `create` / `delete` / `getAll` - создают, удаляют и получают все ссылки
- `/<rest>` - тут уже происходит редирект

![](_png/Pasted%20image%2020250622092936.png)

### Первый POD

 Главное отличие pod от контейнера заключается в том, что это может быть абстракция над несколькими контейнерами. То есть мы можем запустить контейнер с PGSQL и контейнер, который будет его бэкапить.  

![](_png/Pasted%20image%2020250622095427.png)

Далее укажем такую конфигурацию, в которой будет:
- находиться наш собственный image с docker-образом приложения
- в `ports` укажем `containerPort`, который нам нужно прокинуть наружу, чтобы иметь доступ для получения статики сайта от NGINX
- обязательно указываем `resources`, в котором у нас стоят лимиты, при достижении которых, наш сервис будет перезапущен

`pod.yml`
```YAML
---
apiVersion: v1
kind: Pod
metadata:
  name: short-app
  labels:
    components: frontend
spec:
  containers:
    - name: short-app
      image: antonlarichev/short-app
      ports:
        - containerPort: 80 # прокидываем наружу порт контейнера с NGINX
      # обязательно указываем максимальные ресурсы, которые может кушать контейнер
      resources:
        limits:
          memory: "128mi" # mi - mb
          cpu: "500m" # m - miliprocessors - величина относительная к процессору, но указывает количество доступной нагрузки
```

### Сервис

Сервис позволяет держать с контейнером постоянную связь. Без сервиса нам придётся полчать связь каждый раз по IP, который может в любой момент поменяться после редеплоя , если сервис уйдёт в memory leak или упадёт. 

Сервисы бывают 4ёх типов:
- Ingress - позволяет получить доступ к контейнеру извне
- NodePort - позволяет прокинуть порт изнутри контейнера наружу 
- ClusterIP
- LoadBalancer - балансирует нагрузку

Пользователь обращается к IP-адресу кластера, где запрос уходит на прокси, который проксирует все запросы от кластера до определённого сервиса. Потом на сервис, который прокидывает порт из POD в мир. 
 
![](_png/Pasted%20image%2020250622154540.png)

Kube-Proxy требует, чтобы в конфиге было понятно, откуда и куда какой порт проксировать. Поэтому нам нужно будет указать в спеках порты прохода запросов.

- `nodePort` говорит, что при обращении к кластеру по данному порту, мы должны прокинуть запрос в определённый сервис (в котором мы указали этот порт)
- `targetPort` говорит нам на какой порт пода мы должны стучаться, когда уже попали в сервис
- `port` - это внутренний порт для кластера, по которому другие поды смогут достучаться до нашего пода

Для доступа извне нам нужна связка `nodePort` и `targetPort`. В нашем случае, `targetPort` будет равен `containerPort` из конфига пода, так как через таргетпорт нам нужно достучаться до приложения извне кластера 

![](_png/Pasted%20image%2020250622155305.png)

Дальше нам останется только привязать сервис к определённому поду через селектор. В качестве селектора будет выступать заданный ранее лейбл в поде

`node-port.yml`
```YAML
---
apiVersion: v1
kind: Service
metadata:
  name: short-app-port
spec:
  type: NodePort
  # порты для доступа к контейнеру извне кластера
  ports:
   - port: 3000
     targetPort: 80
     nodePort: 31200
  # селектор для связи пода и сервиса
  selectors:
    components: frontend
```

### Подключение к контейнеру

Через утилиту `kubectl` мы можем выполнять все операции по работе с кластером.

#### Применение конфига

Через `apply` мы можем применить определённый конфиг к нашему кластеру для выполнения. 

```bash
# запускаем все файлы в папке
kubectl apply -f .

# либо можно запустить отдельный файл
kubectl apply -f pod.yml
```

#### Инспект конфига

Через `get` мы можем проинспектировать определённый элемент нашей системы.

`all` позволяет получить все возможные образы, которые были запущены ранее в определённом кластере

```bash
$ kubectl get all

NAME            READY   STATUS         RESTARTS   AGE
pod/short-app   0/1     ErrImagePull   0          13s

NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          5m25s
service/short-app-port   NodePort    10.110.44.54   <none>        3000:31200/TCP   4s
```

Так же ничто нам не мешает вывести только поды

```bash
$ kubectl get pods

NAME        READY   STATUS    RESTARTS   AGE
short-app   1/1     Running   0          3m43s
```

Ну и так же отдельно можно взглянуть на запущенные сервисы

```bash
$ kubectl get services

NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          11m
short-app-port   NodePort    10.110.44.54   <none>        3000:31200/TCP   6m8s
```

#### Коннект к контейнеру

Чтобы отправить запрос в кластер, нам нужно будет получить ip кластера, который мы создали через `minikube`

```bash
$ minikube ip

192.168.58.2
```

Теперь можно по `http://192.168.58.2:31200` получить доступ к фронту, который был поднят из кубера

![](_png/Pasted%20image%2020250622164853.png)

### Как работает запуск

`kubectl` отправил запрос в Master ноду в API Service и передал в него конфиг

![](_png/Pasted%20image%2020250622164927.png)

Дальше планировщик ищет ноду, где он сможет запустить наш конфиг `short-app`. У нас пока одна нода. 

![](_png/Pasted%20image%2020250622170107.png)

Дальше уже Controller Manager зафиксировал тот факт, что у нас должен быть один инстанс `short-app`. 
Если наше приложение упадёт, то CM попросит запланировать Scheduler снова поднять контейнер в ноде.

На этом же шаге kubelet (администрирующий нашу ноду) стягивает image из dockerhub и поднимает POD

![](_png/Pasted%20image%2020250622170338.png)

Со стороны Pod у нас всего несколько этапов:
- Scheduled - запланирован 
- Pull - стягивает image, если его нет локально, чтобы запустить 
- Create - создаётся Pod
- Start - запускается Pod с контейнером

Чтобы получить полностью всю информацию о запуске пода, мы можем воспользоваться `kubectl describe`, где есть вся информация по подах
 
```bash
$ kubectl describe pods short-app

Name:             short-app
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.58.2
Start Time:       Sun, 22 Jun 2025 16:30:10 +0300
Labels:           components=frontend
Annotations:      <none>
Status:           Running
IP:               10.244.0.3
IPs:
  IP:  10.244.0.3
Containers:
  short-app:
    Container ID:   docker://465ab518889b248c72f6aaad19df0bef319f53a7edc3d9ff4fd105e2e9b1d378
    Image:          antonlarichev/short-app
    Image ID:       docker-pullable://antonlarichev/short-app@sha256:ecf6b7afbfc7b40b27516953c5dffc7325d5fe95ce811f43faa064ed2c86dcd9
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sun, 22 Jun 2025 16:30:39 +0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        500m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4c4 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-jr4c4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  40m                default-scheduler  Successfully assigned default/short-app to minikube
  Warning  Failed     40m                kubelet            Failed to pull image "antonlarichev/short-app": Error response from daemon: Get "https://registry-1.docker.io/v2/": tls: received record with version 857 when expecting version 303
  Warning  Failed     40m                kubelet            Error: ErrImagePull
  Normal   BackOff    40m                kubelet            Back-off pulling image "antonlarichev/short-app"
  Warning  Failed     40m                kubelet            Error: ImagePullBackOff
  Normal   Pulling    40m (x2 over 40m)  kubelet            Pulling image "antonlarichev/short-app"
  Normal   Pulled     40m                kubelet            Successfully pulled image "antonlarichev/short-app" in 14.328s (14.328s including waiting). Image size: 109145336 bytes.
  Normal   Created    40m                kubelet            Created container: short-app
  Normal   Started    40m                kubelet            Started container short-app
```

Уже ближе к концу виднеется поле Events, в котором описаны все операции и ошибки, которые могли произойти во время сборки. Тут можно заметить, что первые несколько операций прошли с ошибкой и не получилось сразу стянуть с registry валидные данные. Потом уже только после бэк-оффа произошёл пуллинг.

Так же мы можем проинспектировать и сервис. Тут уже меньше информации по айтему, но всё же тут указаны открытые порты, тип, имя и остальные параметры, по которым можно определить точки доступа для запроса

```bash
$ kubectl describe service short-app-port

Name:                     short-app-port
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 components=frontend
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.110.44.54
IPs:                      10.110.44.54
Port:                     <unset>  3000/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  31200/TCP
Endpoints:                10.244.0.3:80
Session Affinity:         None
External Traffic Policy:  Cluster
Internal Traffic Policy:  Cluster
Events:                   <none>
```



---
## Работа с объектами

### Императивный подход

Если нам нужно что-то быстро протестировать или изменить быстро в кластере, то можно воспользоваться императивным подходом и накатить изменения в архитектуру через командную строку командой

```bash
kubectl run my-pod --image=antonlarichev/short-app --labels="component=backend" 
```

Командой `delete` мы можем удалить ненужный нам Pod

```bash
$ kubectl delete pod my-pod

pod "my-pod" deleted
```

### Обновление объектов












### Deployments












### Использование Deployments
### Масштабирование Deployments
### Обновление Image
### Rollout







---
## Volumes








---
## Секреты








---
## Эксплуатация








---
## Знакомство с Helm








---
## Шаблоны








---
## Продвинутые шаблоны








---
## Управление репозиторием








---
## Использование Charts








---
## Заключение












